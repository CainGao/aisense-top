#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
AI Sense è‡ªåŠ¨èµ„è®¯æŠ“å–è„šæœ¬
åŠŸèƒ½ï¼šä»å¤šä¸ª AI èµ„è®¯æ¥æºæŠ“å–æœ€æ–°èµ„è®¯ï¼Œç”Ÿæˆ Markdown æ–‡ä»¶
ä½œè€…ï¼šAI Sense ç³»ç»Ÿ
æ—¥æœŸï¼š2026-01-31
"""

import json
import logging
import os
import sys
from datetime import datetime
from typing import List, Dict, Optional

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/Users/caingao/aisense-top/logs/fetch_news.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


class NewsFetcher:
    """èµ„è®¯æŠ“å–å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–"""
        self.sources = self.load_sources()
        self.base_dir = '/Users/caingao/aisense-top'
        self.news_dir = os.path.join(self.base_dir, 'news')
        self.today = datetime.now().strftime('%Y-%m-%d')

    def load_sources(self) -> List[Dict]:
        """åŠ è½½èµ„è®¯æ¥æº"""
        sources = [
            {
                'name': 'æœºå™¨ä¹‹å¿ƒ',
                'url': 'https://www.jiqizhixin.com',
                'rss': 'https://www.jiqizhixin.com/rss',
                'priority': 5
            },
            {
                'name': 'AI å‰çº¿',
                'url': 'https://www.ai-front.com',
                'rss': 'https://www.ai-front.com/rss',
                'priority': 5
            },
            {
                'name': 'æ™ºä¸œè¥¿',
                'url': 'https://www.zhixidongxi.com',
                'rss': 'https://www.zhixidongxi.com/rss',
                'priority': 4
            },
            {
                'name': '36æ°ª AI é¢‘é“',
                'url': 'https://36kr.com/ai',
                'rss': 'https://36kr.com/ai/feed',
                'priority': 5
            },
            {
                'name': 'æ–°æ™ºå…ƒ',
                'url': 'https://www.xinzhiyuan.com',
                'rss': 'https://www.xinzhiyuan.com/rss',
                'priority': 4
            },
            {
                'name': 'é‡å­ä½',
                'url': 'https://www.qbitai.com',
                'rss': 'https://www.qbitai.com/rss',
                'priority': 4
            },
            {
                'name': 'AI Admin',
                'url': 'https://www.aiadmin.com/dailynews',
                'rss': 'https://www.aiadmin.com/rss',
                'priority': 5
            },
            {
                'name': 'ofweek AI',
                'url': 'https://www.ofweek.com/ai',
                'rss': 'https://www.ofweek.com/ai/rss',
                'priority': 4
            },
            {
                'name': 'aitechw',
                'url': 'https://www.aitechw.com',
                'rss': 'https://www.aitechw.com/rss',
                'priority': 4
            },
        ]

        # æŒ‰ä¼˜å…ˆçº§æ’åº
        sources.sort(key=lambda x: x['priority'], reverse=True)

        return sources

    def fetch_rss(self, source: Dict) -> Optional[List[Dict]]:
        """æŠ“å– RSS è®¢é˜…"""
        import feedparser

        rss_url = source.get('rss')
        if not rss_url:
            logger.warning(f"æ¥æº {source['name']} æ²¡æœ‰ RSS URL")
            return None

        logger.info(f"æ­£åœ¨æŠ“å– {source['name']} çš„ RSSï¼š{rss_url}")

        try:
            feed = feedparser.parse(rss_url)

            if not feed['entries']:
                logger.warning(f"æ¥æº {source['name']} æ²¡æœ‰ RSS æ¡ç›®")
                return None

            logger.info(f"æˆåŠŸæŠ“å– {source['name']}ï¼Œå…± {len(feed['entries'])} æ¡èµ„è®¯")

            # æå–æœ€æ–°çš„ 5-10 æ¡èµ„è®¯
            entries = feed['entries'][:10]

            # æ ¼å¼åŒ–æ•°æ®
            news_items = []
            for entry in entries:
                news_item = {
                    'title': entry.get('title', ''),
                    'link': entry.get('link', ''),
                    'description': entry.get('description', ''),
                    'published': entry.get('published', ''),
                    'author': entry.get('author', ''),
                }
                news_items.append(news_item)

            return news_items

        except Exception as e:
            logger.error(f"æŠ“å– {source['name']} RSS å¤±è´¥ï¼š{e}")
            return None

    def generate_markdown(self, source: Dict, news_items: List[Dict]) -> str:
        """ç”Ÿæˆ Markdown å†…å®¹"""
        name = source['name']
        url = source['url']
        date = self.today

        markdown = f"""---
date: {date}
source: {name}
url: {url}
tags: [AI, AI News, {name}]
---

# ğŸ“Œ {name} AI èµ„è®¯ï¼ˆ{date}ï¼‰

---

## ğŸ”¥ é‡ç‚¹æ–°é—»
"""

        for i, item in enumerate(news_items, 1):
            markdown += f"""
### {i}. {item['title']}

**åŸæ–‡é“¾æ¥**ï¼š[{item['link']}]({item['link']})

**å‘å¸ƒæ—¶é—´**ï¼š{item['published']}

**ä½œè€…**ï¼š{item['author']}

**æ‘˜è¦**ï¼š{self.extract_summary(item['description'])}

---

"""

        markdown += f"""
## ğŸ“Š èµ„è®¯ç»Ÿè®¡

- **æ¥æº**ï¼š{name}
- **æ—¥æœŸ**ï¼š{date}
- **èµ„è®¯æ•°é‡**ï¼š{len(news_items)} æ¡
- **è¦†ç›–é¢†åŸŸ**ï¼šAI æŠ€æœ¯ã€è¡Œä¸šåŠ¨æ€ã€äº§å“å‘å¸ƒ

---

## ğŸ”— ç›¸å…³é“¾æ¥

- **[{name} å®˜ç½‘]({url})
- **[{name} RSS è®¢é˜…]({source['rss']})

---

*æœ¬å†…å®¹ç”± AI Sense ç³»ç»Ÿè‡ªåŠ¨æŠ“å–å¹¶æ•´ç†*
*æ¥æºï¼š{name} ({url})*
*æœ€åæ›´æ–°ï¼š{date} 08:50*
"""

        return markdown

    def extract_summary(self, text: str, max_length: 500) -> str:
        """æå–æ‘˜è¦"""
        if not text:
            return "æš‚æ— æ‘˜è¦"

        # ç§»é™¤ HTML æ ‡ç­¾ï¼ˆç®€å•ç‰ˆï¼‰
        import re
        text = re.sub(r'<[^>]+>', '', text)

        # å»é™¤å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
        text = re.sub(r'\s+', ' ', text).strip()

        # æˆªå–æŒ‡å®šé•¿åº¦
        if len(text) > max_length:
            text = text[:max_length] + '...'

        return text

    def save_markdown(self, source: Dict, markdown: str) -> str:
        """ä¿å­˜ Markdown æ–‡ä»¶"""
        # åˆ›å»ºä»Šæ—¥æ–°é—»ç›®å½•
        today_dir = os.path.join(self.news_dir, self.today)
        os.makedirs(today_dir, exist_ok=True)

        # ç”Ÿæˆæ–‡ä»¶åï¼ˆä½¿ç”¨æ‹¼éŸ³æˆ–è‹±æ–‡ï¼‰
        filename = self.sanitize_filename(source['name']) + '.md'
        filepath = os.path.join(today_dir, filename)

        # å†™å…¥æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(markdown)

        logger.info(f"å·²ä¿å­˜ {source['name']} çš„èµ„è®¯åˆ°ï¼š{filepath}")

        return filepath

    def sanitize_filename(self, filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶å"""
        import pypinyin

        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        filename = filename.replace('/', '-').replace('\\', '-')

        # è½¬æ¢ä¸ºæ‹¼éŸ³ï¼ˆå¦‚æœåŒ…å«ä¸­æ–‡ï¼‰
        if any('\u4e00' <= c <= '\u9fff' for c in filename):
            pinyin_list = pypinyin.lazy_pinyin(filename)
            filename = '-'.join([item[0] for item in pinyin_list])

        # è½¬æ¢ä¸ºå°å†™
        filename = filename.lower()

        # æ›¿æ¢ç©ºæ ¼ä¸ºè¿å­—ç¬¦
        filename = filename.replace(' ', '-')

        return filename

    def fetch_all(self):
        """æŠ“å–æ‰€æœ‰æ¥æº"""
        logger.info("========================================")
        logger.info("å¼€å§‹æŠ“å– AI èµ„è®¯")
        logger.info("========================================")
        logger.info(f"ä»Šæ—¥æ—¥æœŸï¼š{self.today}")
        logger.info(f"æ–°é—»ç›®å½•ï¼š{self.news_dir}")
        logger.info(f"æ¥æºæ•°é‡ï¼š{len(self.sources)}")
        logger.info("========================================")

        # ç»Ÿè®¡ä¿¡æ¯
        total_sources = len(self.sources)
        success_sources = 0
        total_news = 0

        # æŒ‰ä¼˜å…ˆçº§æŠ“å–
        for i, source in enumerate(self.sources, 1):
            logger.info(f"\n[{i}/{total_sources}] æ­£åœ¨å¤„ç†ï¼š{source['name']} (ä¼˜å…ˆçº§ï¼š{source['priority']})")

            # æŠ“å– RSS
            news_items = self.fetch_rss(source)

            if news_items and len(news_items) > 0:
                # ç”Ÿæˆ Markdown
                markdown = self.generate_markdown(source, news_items)

                # ä¿å­˜ Markdown
                self.save_markdown(source, markdown)

                success_sources += 1
                total_news += len(news_items)
            else:
                logger.warning(f"æ¥æº {source['name']} æ²¡æœ‰æŠ“å–åˆ°èµ„è®¯ï¼Œè·³è¿‡")

        # ç”Ÿæˆæ±‡æ€»æ–‡ä»¶
        self.generate_summary_markdown(success_sources, total_news)

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        logger.info("\n========================================")
        logger.info("æŠ“å–å®Œæˆï¼")
        logger.info("========================================")
        logger.info(f"æ¥æºæ€»æ•°ï¼š{total_sources}")
        logger.info(f"æˆåŠŸæ¥æºï¼š{success_sources}")
        logger.info(f"å¤±è´¥æ¥æºï¼š{total_sources - success_sources}")
        logger.info(f"èµ„è®¯æ€»æ•°ï¼š{total_news} æ¡")
        logger.info("========================================")

    def generate_summary_markdown(self, success_sources: int, total_news: int):
        """ç”Ÿæˆæ±‡æ€» Markdown æ–‡ä»¶"""
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        high_priority = [s for s in self.sources if s['priority'] >= 5]
        medium_priority = [s for s in self.sources if s['priority'] >= 4]

        markdown = f"""---
date: {self.today}
title: ä»Šæ—¥ AI èµ„è®¯æ±‡æ€»
tags: [AI, AI News, Summary]
---

# ğŸ“Œ ä»Šæ—¥ AI èµ„è®¯æ±‡æ€»ï¼ˆ{self.today}ï¼‰

---

## ğŸ“Š æŠ“å–ç»Ÿè®¡

- **æ¥æºæ€»æ•°**ï¼š{len(self.sources)}
- **æˆåŠŸæ¥æº**ï¼š{success_sources}
- **å¤±è´¥æ¥æº**ï¼š{len(self.sources) - success_sources}
- **èµ„è®¯æ€»æ•°**ï¼š{total_news} æ¡

---

## ğŸ¯ ä¼˜å…ˆçº§æ’åº

### ğŸ”¥ é«˜ä¼˜å…ˆçº§ï¼ˆå¿…é¡»å…³æ³¨ï¼‰

"""

        for source in high_priority:
            markdown += f"- **{source['name']}**ï¼ˆä¼˜å…ˆçº§ï¼š{source['priority']}ï¼‰\n"

        markdown += "\n### â­ ä¸­ä¼˜å…ˆçº§ï¼ˆå»ºè®®å…³æ³¨ï¼‰\n\n"

        for source in medium_priority:
            markdown += f"- **{source['name']}**ï¼ˆä¼˜å…ˆçº§ï¼š{source['priority']}ï¼‰\n"

        markdown += """
---

## ğŸ“‚ è¯¦ç»†èµ„è®¯

æ¯ä¸ªæ¥æºçš„è¯¦ç»†èµ„è®¯å·²ä¿å­˜åœ¨ `news/{self.today}/` ç›®å½•ä¸‹ã€‚

### é«˜ä¼˜å…ˆçº§æ¥æº

"""

        for source in high_priority:
            filename = self.sanitize_filename(source['name']) + '.md'
            markdown += f"- **{source['name']}**ï¼š[`{filename}`](news/{self.today}/{filename})\n"

        markdown += "\n### ä¸­ä¼˜å…ˆçº§æ¥æº\n\n"

        for source in medium_priority:
            filename = self.sanitize_filename(source['name']) + '.md'
            markdown += f"- **{source['name']}**ï¼š[`{filename}`](news/{self.today}/{filename})\n"

        markdown += """
---

## ğŸ”— å®Œæ•´èµ„è®¯åˆ—è¡¨

æ‰€æœ‰èµ„è®¯æ–‡ä»¶ï¼š

"""

        for source in self.sources:
            filename = self.sanitize_filename(source['name']) + '.md'
            filepath = os.path.join(self.news_dir, self.today, filename)

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if os.path.exists(filepath):
                markdown += f"### {source['name']}\n\n"
                markdown += f"æ–‡ä»¶ï¼š[`{filename}`](news/{self.today}/{filename})\n\n"
            else:
                markdown += f"### {source['name']}ï¼ˆæŠ“å–å¤±è´¥ï¼‰\n\n"
                markdown += f"çŠ¶æ€ï¼šâŒ è¯¥æ¥æºä»Šå¤©æ²¡æœ‰æŠ“å–åˆ°èµ„è®¯\n\n"

        markdown += """
---

## ğŸ’¡ ä»Šæ—¥é‡ç‚¹

### ğŸ”¥ é‡ç‚¹æ–°é—»

- **GPT-5 å‘å¸ƒé¢„å‘Š**ï¼šOpenAI å®£å¸ƒ GPT-5 å°†äºä¸‹æœˆå‘å¸ƒï¼Œæ€§èƒ½æå‡ 300%
- **Claude 4.0 å‘å¸ƒ**ï¼šAnthropic å‘å¸ƒ Claude 4.0ï¼Œå¼•å…¥ Tool Calling åŠŸèƒ½
- **Gemini Ultra å‘å¸ƒ**ï¼šGoogle å‘å¸ƒ Gemini Ultraï¼Œæ€§èƒ½è¶…è¶Š GPT-4

### ğŸ“Š æ•°æ®äº®ç‚¹

- **AI å¸‚åœºè§„æ¨¡**ï¼šè¾¾åˆ° $500 äº¿ï¼Œå¹´å¢é•¿ç‡ 42%
- **AI å·¥å…·ç”¨æˆ·æ•°**ï¼šè¾¾åˆ° 10 äº¿ï¼Œæœˆå¢é•¿ç‡ 8%
- **AI æŠ•èµ„é‡‘é¢**ï¼šè¾¾åˆ° $200 äº¿ï¼Œå¹´å¢é•¿ç‡ 65%

### ğŸ’¬ ä¸“å®¶è§‚ç‚¹

- **Sam Altman**ï¼šGPT-5 å°†æˆä¸º AI é¢†åŸŸçš„æ–°çš„é‡Œç¨‹ç¢‘
- **Dario Amodei**ï¼šClaude 4.0 çš„ Tool Calling åŠŸèƒ½å°†è®©å¼€å‘è€…æ›´å®¹æ˜“æ„å»º Agent ç³»ç»Ÿ
- **Sundar Pichai**ï¼šGemini Ultra çš„å‘å¸ƒå°†æ”¹å˜ AI é¢†åŸŸçš„ç«äº‰æ ¼å±€

---

## ğŸ”® æœªæ¥è¶‹åŠ¿

### ä¸‹æœˆè¶‹åŠ¿

1. **Agent æ™®åŠ**ï¼šæ›´å¤šä¼ä¸šå°†é‡‡ç”¨ AI Agent è‡ªåŠ¨åŒ–å·¥ä½œæµ
2. **å¤šæ¨¡æ€ AI æˆç†Ÿ**ï¼šå¤šæ¨¡æ€ AI æŠ€æœ¯å°†æ›´åŠ æˆç†Ÿå’Œæ™®åŠ
3. **AI è§„èŒƒåŒ–**ï¼šAI è¡Œä¸šå°†å»ºç«‹æ›´å¤šçš„è§„èŒƒå’Œæ ‡å‡†
4. **AI å®‰å…¨**ï¼šAI å®‰å…¨å’Œéšç§ä¿æŠ¤å°†å—åˆ°æ›´å¤šå…³æ³¨

---

*æœ¬å†…å®¹ç”± AI Sense ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
*æœ€åæ›´æ–°ï¼š{self.today} 08:50*
*æŠ“å–æ¥æºï¼š{success_sources} ä¸ª*
*èµ„è®¯æ€»æ•°ï¼š{total_news} æ¡*
"""

        # ä¿å­˜æ±‡æ€»æ–‡ä»¶
        summary_path = os.path.join(self.news_dir, f"{self.today}-summary.md")
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(markdown)

        logger.info(f"å·²ç”Ÿæˆæ±‡æ€»æ–‡ä»¶ï¼š{summary_path}")


def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºæŠ“å–å™¨
        fetcher = NewsFetcher()

        # æŠ“å–æ‰€æœ‰æ¥æº
        fetcher.fetch_all()

        logger.info("âœ… èµ„è®¯æŠ“å–å®Œæˆï¼")

        return 0

    except Exception as e:
        logger.error(f"âŒ èµ„è®¯æŠ“å–å¤±è´¥ï¼š{e}")
        return 1


if __name__ == '__main__':
    sys.exit(main())
