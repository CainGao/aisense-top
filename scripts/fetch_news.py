#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
AI Sense æ™ºèƒ½èµ„è®¯æŠ“å–è„šæœ¬ï¼ˆå‡çº§ç‰ˆï¼‰
åŠŸèƒ½ï¼šå¤šæºæŠ“å–ã€æ™ºèƒ½å»é‡ã€åŒç±»åˆå¹¶ã€ä»·å€¼è¿‡æ»¤
ä½œè€…ï¼šAI Sense ç³»ç»Ÿ
æ—¥æœŸï¼š2026-01-31
"""

import json
import logging
import os
import sys
import hashlib
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Set, Tuple
from difflib import SequenceMatcher

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/Users/caingao/aisense-top/logs/fetch_news.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


class IntelligentNewsFetcher:
    """æ™ºèƒ½èµ„è®¯æŠ“å–å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–"""
        self.sources = self.load_sources()
        self.base_dir = '/Users/caingao/aisense-top'
        self.news_dir = os.path.join(self.base_dir, 'news')
        self.database_file = os.path.join(self.base_dir, 'database', 'news_database.json')
        self.today = datetime.now().strftime('%Y-%m-%d')

        # åŠ è½½æ•°æ®åº“
        self.database = self.load_database()

        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_fetched': 0,
            'unique_news': 0,
            'duplicate_news': 0,
            'merged_news': 0,
            'low_value_news': 0,
            'high_value_news': 0,
        }

    def load_sources(self) -> List[Dict]:
        """åŠ è½½èµ„è®¯æ¥æº"""
        sources = [
            {
                'name': 'æœºå™¨ä¹‹å¿ƒ',
                'url': 'https://www.jiqizhixin.com',
                'rss': 'https://www.jiqizhixin.com/rss',
                'priority': 5,
                'weight': 40  # æ¥æºæƒé‡ï¼ˆä»·å€¼è¯„åˆ†ï¼‰
            },
            {
                'name': 'AI å‰çº¿',
                'url': 'https://www.ai-front.com',
                'rss': 'https://www.ai-front.com/rss',
                'priority': 5,
                'weight': 40
            },
            {
                'name': 'æ™ºä¸œè¥¿',
                'url': 'https://www.zhixidongxi.com',
                'rss': 'https://www.zhixidongxi.com/rss',
                'priority': 4,
                'weight': 30
            },
            {
                'name': '36æ°ª AI é¢‘é“',
                'url': 'https://36kr.com/ai',
                'rss': 'https://36kr.com/ai/feed',
                'priority': 5,
                'weight': 40
            },
            {
                'name': 'æ–°æ™ºå…ƒ',
                'url': 'https://www.xinziyuan.com',
                'rss': 'https://www.xinziyuan.com/rss',
                'priority': 4,
                'weight': 30
            },
            {
                'name': 'é‡å­ä½',
                'url': 'https://www.qbitai.com',
                'rss': 'https://www.qbitai.com/rss',
                'priority': 4,
                'weight': 30
            },
            {
                'name': 'AI Admin',
                'url': 'https://www.aiadmin.com/dailynews',
                'rss': 'https://www.aiadmin.com/rss',
                'priority': 5,
                'weight': 40
            },
            {
                'name': 'ofweek AI',
                'url': 'https://www.ofweek.com/ai',
                'rss': 'https://www.ofweek.com/ai/rss',
                'priority': 4,
                'weight': 30
            },
            {
                'name': 'aitechw',
                'url': 'https://www.aitechw.com',
                'rss': 'https://www.aitechw.com/rss',
                'priority': 4,
                'weight': 30
            },
        ]

        # æŒ‰ä¼˜å…ˆçº§å’Œæƒé‡æ’åº
        sources.sort(key=lambda x: (x['priority'], x['weight']), reverse=True)

        return sources

    def load_database(self) -> Dict:
        """åŠ è½½æ•°æ®åº“"""
        if not os.path.exists(self.database_file):
            logger.info("æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°æ•°æ®åº“...")
            return self.create_database()

        with open(self.database_file, 'r', encoding='utf-8') as f:
            database = json.load(f)

        logger.info(f"æ•°æ®åº“å·²åŠ è½½ï¼ŒåŒ…å« {len(database['news'])} æ¡æ–°é—»")
        return database

    def create_database(self) -> Dict:
        """åˆ›å»ºæ–°æ•°æ®åº“"""
        database = {
            'metadata': {
                'version': '1.0',
                'last_updated': datetime.now().isoformat(),
                'total_news': 0,
                'unique_news': 0,
                'duplicate_news': 0,
                'merged_news': 0,
                'low_value_news': 0,
                'high_value_news': 0,
            },
            'news': {}
        }

        # ä¿å­˜æ•°æ®åº“
        self.save_database(database)

        return database

    def save_database(self):
        """ä¿å­˜æ•°æ®åº“"""
        # æ›´æ–°å…ƒæ•°æ®
        self.database['metadata']['last_updated'] = datetime.now().isoformat()
        self.database['metadata']['total_news'] = len(self.database['news'])

        # ä¿å­˜åˆ°æ–‡ä»¶
        os.makedirs(os.path.dirname(self.database_file), exist_ok=True)
        with open(self.database_file, 'w', encoding='utf-8') as f:
            json.dump(self.database, f, ensure_ascii=False, indent=2)

        logger.info(f"æ•°æ®åº“å·²ä¿å­˜ï¼ŒåŒ…å« {len(self.database['news'])} æ¡æ–°é—»")

    def is_duplicate(self, news_item: Dict) -> Tuple[bool, Optional[str]]:
        """
        æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤æ–°é—»

        è¿”å›: (æ˜¯å¦é‡å¤, å»é‡ç±»å‹)
        """
        # ç¬¬ä¸€çº§ï¼šåŸºäºé“¾æ¥å»é‡
        link_hash = self.calculate_hash(news_item['url'])
        if link_hash in self.database['news']:
            logger.info(f"é“¾æ¥å·²å­˜åœ¨ï¼š{news_item['title']}")
            return True, 'link'

        # ç¬¬äºŒçº§ï¼šåŸºäºæ ‡é¢˜å»é‡
        title_hash = self.calculate_hash(news_item['title'])
        if title_hash in self.database['news']:
            logger.info(f"æ ‡é¢˜å·²å­˜åœ¨ï¼š{news_item['title']}")
            return True, 'title'

        # ç¬¬ä¸‰çº§ï¼šåŸºäºå†…å®¹å»é‡ï¼ˆå¯é€‰ï¼Œç”±äºè®¡ç®—é‡å¤§ï¼‰
        # å¦‚æœå¯ç”¨ï¼Œå¯ä»¥å–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
        # content_hash = self.calculate_hash(news_item.get('description', ''))
        # if content_hash in self.database['news']:
        #     logger.info(f"å†…å®¹å·²å­˜åœ¨ï¼š{news_item['title']}")
        #     return True, 'content'

        return False, None

    def calculate_hash(self, text: str) -> str:
        """è®¡ç®—å“ˆå¸Œå€¼"""
        if not text:
            return hashlib.md5(b'').hexdigest()

        return hashlib.md5(text.encode('utf-8')).hexdigest()

    def find_similar_news(self, news_item: Dict, threshold: float = 0.7) -> List[Dict]:
        """
        æŸ¥æ‰¾ç›¸ä¼¼æ–°é—»ï¼ˆç”¨äºåˆå¹¶ï¼‰

        è¿”å›: ç›¸ä¼¼æ–°é—»åˆ—è¡¨
        """
        # æå–å…³é”®è¯
        keywords = self.extract_keywords(news_item['title'])

        # éå†æ‰€æœ‰æ–°é—»
        similar_news = []

        for news_id, news in self.database['news'].items():
            # æå–å…³é”®è¯
            existing_keywords = self.extract_keywords(news['title'])

            # è®¡ç®— Jaccard ç›¸ä¼¼åº¦
            similarity = self.calculate_jaccard(keywords, existing_keywords)

            # å¦‚æœç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼ï¼Œæ·»åŠ åˆ°ç»“æœ
            if similarity >= threshold:
                similar_news.append({
                    'id': news_id,
                    'similarity': similarity,
                    'news': news
                })

        # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼ˆé™åºï¼‰
        similar_news.sort(key=lambda x: x['similarity'], reverse=True)

        return similar_news

    def calculate_jaccard(self, set1: Set, set2: Set) -> float:
        """
        è®¡ç®— Jaccard ç›¸ä¼¼åº¦

        å…¬å¼: J(A, B) = |A âˆ© B| / |A âˆª B|
        """
        if not set1 or not set2:
            return 0.0

        intersection = len(set1 & set2)
        union = len(set1 | set2)

        return intersection / union if union > 0 else 0.0

    def extract_keywords(self, text: str) -> Set[str]:
        """æå–å…³é”®è¯"""
        if not text:
            return set()

        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        import re
        text = re.sub(r'[^\w\s\u4e00-\u9fff]', '', text)

        # åˆ†å‰²æˆå•è¯
        words = text.split()

        # ç§»é™¤åœç”¨è¯ï¼ˆç®€å•ç‰ˆï¼‰
        stop_words = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'å’Œ', 'ä¸', 'çš„', 'äº†', 'äº†', 'äº†', 'çš„', 'the', 'a', 'an', 'of', 'to', 'in'}
        keywords = [word.lower() for word in words if len(word) > 1 and word.lower() not in stop_words]

        return set(keywords)

    def evaluate_news_value(self, news_item: Dict, source: Dict) -> Tuple[int, Dict]:
        """
        è¯„ä¼°æ–°é—»ä»·å€¼

        è¿”å›: (æ€»åˆ†, è¯¦ç»†è¯„åˆ†)
        """
        scores = {
            'source_score': 0,
            'title_score': 0,
            'content_score': 0,
            'time_score': 0
        }

        # 1. æ¥æºè¯„åˆ†ï¼ˆ40 åˆ†ï¼‰
        scores['source_score'] = source.get('weight', 20)

        # 2. æ ‡é¢˜è¯„åˆ†ï¼ˆ30 åˆ†ï¼‰
        title = news_item.get('title', '')
        title_lower = title.lower()

        # æ£€æŸ¥å…³é”®è¯
        high_value_keywords = ['gpt-5', 'gpt-6', 'claude 4.0', 'claude 5.0', 'gemini ultra', 'llama 3', 'llama 4', 'agent', 'mcp', 'å¤šæ¨¡æ€', 'ai agent']
        medium_value_keywords = ['å‘å¸ƒ', 'æ›´æ–°', 'ä¸Šçº¿', 'æ¨å‡º', 'å®£å¸ƒ', 'å®Œæˆ', 'è¾¾åˆ°']
        low_value_keywords = ['ä¼˜æƒ ', 'æ´»åŠ¨', 'ä¿ƒé”€', 'æ‰“æŠ˜', 'å…è´¹', 'é™æ—¶', 'ç¦åˆ©']

        high_count = sum(1 for keyword in high_value_keywords if keyword in title_lower)
        medium_count = sum(1 for keyword in medium_value_keywords if keyword in title_lower)
        low_count = sum(1 for keyword in low_value_keywords if keyword in title_lower)

        if high_count >= 1:
            scores['title_score'] = 30
        elif medium_count >= 1:
            scores['title_score'] = 25
        elif low_count >= 1:
            scores['title_score'] = 15
        elif len(title) > 10:
            scores['title_score'] = 10
        else:
            scores['title_score'] = 5

        # 3. å†…å®¹è¯„åˆ†ï¼ˆ20 åˆ†ï¼‰
        description = news_item.get('description', '')
        if len(description) > 500:
            scores['content_score'] = 20
        elif len(description) > 300:
            scores['content_score'] = 15
        elif len(description) > 100:
            scores['content_score'] = 10
        elif len(description) > 0:
            scores['content_score'] = 5
        else:
            scores['content_score'] = 0

        # 4. æ—¶é—´è¯„åˆ†ï¼ˆ10 åˆ†ï¼‰
        published = news_item.get('published', '')
        if published:
            try:
                # å°è¯•è§£æå‘å¸ƒæ—¶é—´
                pub_datetime = datetime.fromisoformat(published.replace('Z', '+00:00'))
                time_diff = datetime.now(pub_datetime.tzinfo) - pub_datetime

                if time_diff.total_seconds() < 86400:  # 24å°æ—¶å†…
                    scores['time_score'] = 10
                elif time_diff.total_seconds() < 259200:  # 3å¤©å†…
                    scores['time_score'] = 8
                elif time_diff.total_seconds() < 604800:  # 7å¤©å†…
                    scores['time_score'] = 6
                elif time_diff.total_seconds() < 2592000:  # 30å¤©å†…
                    scores['time_score'] = 4
                else:
                    scores['time_score'] = 2
            except:
                scores['time_score'] = 5
        else:
            scores['time_score'] = 10  # å‡è®¾æ˜¯æœ€æ–°æ–°é—»

        # è®¡ç®—æ€»åˆ†
        total_score = (
            scores['source_score'] +
            scores['title_score'] +
            scores['content_score'] +
            scores['time_score']
        )

        # è®¡ç®—ä¼˜å…ˆçº§
        if total_score >= 80:
            priority = 5  # æœ€é«˜ä¼˜å…ˆçº§
        elif total_score >= 70:
            priority = 4  # é«˜ä¼˜å…ˆçº§
        elif total_score >= 60:
            priority = 3  # ä¸­ä¼˜å…ˆçº§
        elif total_score >= 50:
            priority = 2  # ä½ä¼˜å…ˆçº§
        else:
            priority = 1  # æœ€ä½ä¼˜å…ˆçº§

        return total_score, scores, priority

    def process_news_item(self, news_item: Dict, source: Dict) -> Optional[Dict]:
        """
        å¤„ç†æ–°é—»é¡¹ï¼ˆå»é‡ã€åˆå¹¶ã€ä»·å€¼è¯„ä¼°ï¼‰

        è¿”å›: å¤„ç†åçš„æ–°é—»é¡¹ï¼Œæˆ– Noneï¼ˆå¦‚æœæ˜¯ä½ä»·å€¼æˆ–é‡å¤ï¼‰
        """
        # 1. æ£€æŸ¥æ˜¯å¦é‡å¤
        is_dup, dup_type = self.is_duplicate(news_item)

        if is_dup:
            # å¦‚æœæ˜¯é‡å¤ï¼ŒæŸ¥æ‰¾ç›¸ä¼¼æ–°é—»å¹¶åˆå¹¶
            similar_news = self.find_similar_news(news_item, threshold=0.7)

            if similar_news:
                # åˆå¹¶åˆ°ç›¸ä¼¼æ–°é—»
                merged_news = self.merge_news(similar_news[0]['news'], news_item, source)
                merged_news['status'] = 'merged'
                self.stats['merged_news'] += 1
                self.update_database(merged_news)
                return merged_news
            else:
                # çº¯ç²¹é‡å¤ï¼Œè·³è¿‡
                self.stats['duplicate_news'] += 1
                return None

        # 2. è¯„ä¼°æ–°é—»ä»·å€¼
        total_score, scores, priority = self.evaluate_news_value(news_item, source)

        if total_score < 50:  # ä½ä»·å€¼ï¼Œè¿‡æ»¤
            logger.info(f"ä½ä»·å€¼æ–°é—»ï¼ˆ{total_score} åˆ†ï¼‰ï¼š{news_item['title']}")
            self.stats['low_value_news'] += 1
            return None

        # 3. é«˜ä»·å€¼ï¼Œæ·»åŠ åˆ°æ•°æ®åº“
        news_id = self.calculate_hash(news_item['url'] + news_item['title'])

        news_record = {
            'id': news_id,
            'title': news_item['title'],
            'url': news_item['url'],
            'sources': [{
                'name': source['name'],
                'url': source['url'],
                'published': news_item.get('published', ''),
                'summary': self.extract_summary(news_item.get('description', ''))
            }],
            'value': {
                'score': total_score,
                'scores': scores,
                'priority': priority,
                'category': ['AI Technology', 'E-commerce', 'Media', 'Programming']
            },
            'first_seen': datetime.now().isoformat(),
            'last_seen': datetime.now().isoformat(),
            'seen_count': 1,
            'status': 'new'
        }

        self.stats['high_value_news'] += 1
        self.stats['unique_news'] += 1

        # æ·»åŠ åˆ°æ•°æ®åº“
        self.database['news'][news_id] = news_record

        logger.info(f"é«˜ä»·å€¼æ–°é—»ï¼ˆ{total_score} åˆ†ï¼‰ï¼š{news_item['title']}")

        return news_record

    def merge_news(self, existing_news: Dict, new_news_item: Dict, source: Dict) -> Dict:
        """
        åˆå¹¶æ–°é—»ï¼ˆæ·»åŠ æ–°æ¥æºï¼‰

        è¿”å›: åˆå¹¶åçš„æ–°é—»
        """
        # æ·»åŠ æ–°çš„æ¥æº
        if 'sources' not in existing_news:
            existing_news['sources'] = []
        existing_news['sources'].append({
            'name': source['name'],
            'url': source['url'],
            'published': new_news_item.get('published', ''),
            'summary': self.extract_summary(new_news_item.get('description', ''))
        })

        # æ›´æ–°æœ€åæŠ“å–æ—¶é—´
        existing_news['last_seen'] = datetime.now().isoformat()

        # æ›´æ–°æŠ“å–æ¬¡æ•°
        existing_news['seen_count'] += 1

        # é‡æ–°è¯„ä¼°ä»·å€¼ï¼ˆå¯èƒ½æé«˜ï¼‰
        existing_news['value']['score'] += 5  # å¢åŠ åˆ†æ•°
        if existing_news['value']['score'] > 100:
            existing_news['value']['score'] = 100  # ä¸Šé™ 100

        # æ›´æ–°ä¼˜å…ˆçº§
        if existing_news['value']['score'] >= 80:
            existing_news['value']['priority'] = 5
        elif existing_news['value']['score'] >= 70:
            existing_news['value']['priority'] = 4
        elif existing_news['value']['score'] >= 60:
            existing_news['value']['priority'] = 3
        elif existing_news['value']['score'] >= 50:
            existing_news['value']['priority'] = 2
        else:
            existing_news['value']['priority'] = 1

        return existing_news

    def update_database(self, news_record: Dict):
        """æ›´æ–°æ•°æ®åº“"""
        if news_record and 'id' in news_record:
            self.database['news'][news_record['id']] = news_record

    def fetch_rss(self, source: Dict) -> Optional[List[Dict]]:
        """æŠ“å– RSS è®¢é˜…"""
        import feedparser

        rss_url = source.get('rss')
        if not rss_url:
            logger.warning(f"æ¥æº {source['name']} æ²¡æœ‰ RSS URL")
            return None

        logger.info(f"æ­£åœ¨æŠ“å– {source['name']} çš„ RSSï¼š{rss_url}")

        try:
            feed = feedparser.parse(rss_url)

            if not feed['entries']:
                logger.warning(f"æ¥æº {source['name']} æ²¡æœ‰ RSS æ¡ç›®")
                return None

            logger.info(f"æˆåŠŸæŠ“å– {source['name']}ï¼Œå…± {len(feed['entries'])} æ¡èµ„è®¯")

            # æå–æœ€æ–°çš„ 5-10 æ¡èµ„è®¯
            entries = feed['entries'][:10]

            # æ ¼å¼åŒ–æ•°æ®
            news_items = []
            for entry in entries:
                news_item = {
                    'title': entry.get('title', ''),
                    'link': entry.get('link', ''),
                    'description': entry.get('description', ''),
                    'published': entry.get('published', ''),
                    'author': entry.get('author', ''),
                }
                news_items.append(news_item)

            self.stats['total_fetched'] += len(news_items)

            return news_items

        except Exception as e:
            logger.error(f"æŠ“å– {source['name']} RSS å¤±è´¥ï¼š{e}")
            return None

    def generate_markdown(self, news_record: Dict) -> str:
        """ç”Ÿæˆ Markdown å†…å®¹"""
        name = news_record['sources'][0]['name']
        url = news_record['sources'][0]['url']
        date = self.today

        markdown = f"""---
date: {date}
source: {name}
url: {url}
value_score: {news_record['value']['score']}
priority: {news_record['value']['priority']}
tags: [AI, AI News, {name}, High Value]
---

# ğŸ“Œ {name} AI èµ„è®¯ï¼ˆ{date}ï¼‰

---

## ğŸ”¥ é‡ç‚¹æ–°é—»

### {news_record['title']}

**åŸæ–‡é“¾æ¥**ï¼š[{news_record['url']}]({news_record['url']})

**æ¥æº**ï¼š
"""

        # æ·»åŠ æ‰€æœ‰æ¥æº
        for i, source in enumerate(news_record['sources'], 1):
            markdown += f"- **æ¥æº {i}**ï¼š[{source['name']}]({source['url']}) - [{source['published']}]({source['url']})\n"

        markdown += f"""
**å‘å¸ƒæ—¶é—´**ï¼š{news_record['sources'][0]['published']}

**æ‘˜è¦**ï¼š{news_record['sources'][0]['summary']}

**ä»·å€¼è¯„åˆ†**ï¼š{news_record['value']['score']} / 100

**ä¼˜å…ˆçº§**ï¼š{news_record['value']['priority']} / 5

---

## ğŸ“Š è¯„åˆ†è¯¦æƒ…

### æ¥æºè¯„åˆ†ï¼š{news_record['value']['scores']['source_score']} / 40
- {news_record['sources'][0]['name']}ï¼š{news_record['value']['scores']['source_score']} åˆ†

### æ ‡é¢˜è¯„åˆ†ï¼š{news_record['value']['scores']['title_score']} / 30
- æ ‡é¢˜åŒ…å«å…³é”®è¯

### å†…å®¹è¯„åˆ†ï¼š{news_record['value']['scores']['content_score']} / 20
- å†…å®¹é•¿åº¦å’Œè´¨é‡

### æ—¶é—´è¯„åˆ†ï¼š{news_record['value']['scores']['time_score']} / 10
- å‘å¸ƒæ—¶é—´æ–°é²œåº¦

---

## ğŸ”— ç›¸å…³é“¾æ¥

- [{name} å®˜ç½‘]({url})
- [{name} RSS è®¢é˜…]({news_record['sources'][0]['url']})

---

## ğŸ’¡ ä»·å€¼åˆ†æ

### ä¸ºä»€ä¹ˆè¿™æ¡æ–°é—»é‡è¦ï¼Ÿ

1. **æ¥æºæƒå¨**ï¼š{news_record['sources'][0]['name']} æ˜¯æƒå¨çš„ AI èµ„è®¯åª’ä½“
2. **å†…å®¹é«˜è´¨é‡**ï¼šæ–°é—»å†…å®¹è¯¦ç»†ï¼ŒåŒ…å«æ•°æ®å’Œåˆ†æ
3. **æ—¶æ•ˆæ€§å¼º**ï¼šæ–°é—»å‘å¸ƒæ—¶é—´è¾ƒæ–°
4. **å½±å“å¹¿æ³›**ï¼šæ–°é—»å¯¹ AI è¡Œä¸šæœ‰é‡è¦å½±å“

### å¯¹è¯»è€…çš„ä»·å€¼

- äº†è§£æœ€æ–° AI è¡Œä¸šåŠ¨æ€
- è·å–æƒå¨çš„è¡Œä¸šä¿¡æ¯
- åšå‡ºæ›´æ˜æ™ºçš„å†³ç­–

---

*æœ¬å†…å®¹ç”± AI Sense ç³»ç»Ÿè‡ªåŠ¨æŠ“å–å¹¶æ•´ç†*
*æ¥æºï¼š{name} ({url})*
*ä»·å€¼è¯„åˆ†ï¼š{news_record['value']['score']} / 100*
*æœ€åæ›´æ–°ï¼š{date} 08:50*
"""

        return markdown

    def extract_summary(self, text: str, max_length: 500) -> str:
        """æå–æ‘˜è¦"""
        if not text:
            return "æš‚æ— æ‘˜è¦"

        # ç§»é™¤ HTML æ ‡ç­¾ï¼ˆç®€å•ç‰ˆï¼‰
        import re
        text = re.sub(r'<[^>]+>', '', text)

        # ç§»é™¤å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
        text = re.sub(r'\s+', ' ', text).strip()

        # æˆªå–æŒ‡å®šé•¿åº¦
        if len(text) > max_length:
            text = text[:max_length] + '...'

        return text

    def sanitize_filename(self, filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶å"""
        import pypinyin

        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        filename = filename.replace('/', '-').replace('\\', '-')

        # è½¬æ¢ä¸ºæ‹¼éŸ³ï¼ˆå¦‚æœåŒ…å«ä¸­æ–‡ï¼‰
        if any('\u4e00' <= c <= '\u9fff' for c in filename):
            pinyin_list = pypinyin.lazy_pinyin(filename)
            filename = '-'.join([item[0] for item in pinyin_list])

        # è½¬æ¢ä¸ºå°å†™
        filename = filename.lower()

        # æ›¿æ¢ç©ºæ ¼ä¸ºè¿å­—ç¬¦
        filename = filename.replace(' ', '-')

        # é™åˆ¶æ–‡ä»¶åé•¿åº¦
        if len(filename) > 100:
            filename = filename[:100]

        return filename

    def save_markdown(self, news_record: Dict, markdown: str) -> str:
        """ä¿å­˜ Markdown æ–‡ä»¶"""
        # åˆ›å»ºä»Šæ—¥æ–°é—»ç›®å½•
        today_dir = os.path.join(self.news_dir, self.today)
        os.makedirs(today_dir, exist_ok=True)

        # ç”Ÿæˆæ–‡ä»¶åï¼ˆä½¿ç”¨æ‹¼éŸ³æˆ–è‹±æ–‡ï¼‰
        name = news_record['sources'][0]['name']
        score = news_record['value']['score']
        filename = f"{self.sanitize_filename(name)}-{score}.md"
        filepath = os.path.join(today_dir, filename)

        # å†™å…¥æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(markdown)

        logger.info(f"å·²ä¿å­˜ {name} çš„èµ„è®¯åˆ°ï¼š{filepath}")

        return filepath

    def fetch_all(self):
        """æŠ“å–æ‰€æœ‰æ¥æº"""
        logger.info("========================================")
        logger.info("å¼€å§‹æ™ºèƒ½æŠ“å– AI èµ„è®¯")
        logger.info("========================================")
        logger.info(f"ä»Šæ—¥æ—¥æœŸï¼š{self.today}")
        logger.info(f"æ–°é—»ç›®å½•ï¼š{self.news_dir}")
        logger.info(f"æ•°æ®åº“æ–‡ä»¶ï¼š{self.database_file}")
        logger.info(f"æ¥æºæ•°é‡ï¼š{len(self.sources)}")
        logger.info("========================================")

        # ç»Ÿè®¡ä¿¡æ¯
        high_value_news_records = []

        # æŒ‰ä¼˜å…ˆçº§æŠ“å–
        for i, source in enumerate(self.sources, 1):
            logger.info(f"\n[{i}/{len(self.sources)}] æ­£åœ¨å¤„ç†ï¼š{source['name']} (ä¼˜å…ˆçº§ï¼š{source['priority']}, æƒé‡ï¼š{source['weight']})")

            # æŠ“å– RSS
            news_items = self.fetch_rss(source)

            if news_items and len(news_items) > 0:
                for j, news_item in enumerate(news_items, 1):
                    logger.info(f"  [{j}/{len(news_items)}] å¤„ç†æ–°é—»ï¼š{news_item['title'][:50]}...")

                    # å¤„ç†æ–°é—»é¡¹ï¼ˆå»é‡ã€åˆå¹¶ã€ä»·å€¼è¯„ä¼°ï¼‰
                    processed_news = self.process_news_item(news_item, source)

                    # å¦‚æœæ˜¯é«˜ä»·å€¼æ–°é—»ï¼Œä¿å­˜åˆ°åˆ—è¡¨
                    if processed_news and processed_news['value']['score'] >= 70:
                        high_value_news_records.append(processed_news)
            else:
                logger.warning(f"æ¥æº {source['name']} æ²¡æœ‰æŠ“å–åˆ°èµ„è®¯ï¼Œè·³è¿‡")

        # ç”Ÿæˆæ±‡æ€»æ–‡ä»¶
        if high_value_news_records:
            self.generate_summary_markdown(high_value_news_records)

        # ä¿å­˜æ•°æ®åº“
        self.save_database()

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self.print_statistics()

    def generate_summary_markdown(self, news_records: List[Dict]):
        """ç”Ÿæˆæ±‡æ€» Markdown æ–‡ä»¶"""
        # æŒ‰ä»·å€¼è¯„åˆ†æ’åºï¼ˆé™åºï¼‰
        news_records.sort(key=lambda x: x['value']['score'], reverse=True)

        markdown = f"""---
date: {self.today}
title: ä»Šæ—¥é«˜ä»·å€¼ AI èµ„è®¯æ±‡æ€»
tags: [AI, AI News, Summary, High Value]
---

# ğŸ“Œ ä»Šæ—¥é«˜ä»·å€¼ AI èµ„è®¯æ±‡æ€»ï¼ˆ{self.today}ï¼‰

---

## ğŸ“Š ç»Ÿè®¡ä¿¡æ¯

- **é«˜ä»·å€¼æ–°é—»æ•°é‡**ï¼š{len(news_records)} æ¡ï¼ˆä»·å€¼è¯„åˆ† >= 70ï¼‰
- **æ€»æŠ“å–æ•°é‡**ï¼š{self.stats['total_fetched']} æ¡
- **å”¯ä¸€æ–°é—»æ•°é‡**ï¼š{self.stats['unique_news']} æ¡
- **é‡å¤æ–°é—»æ•°é‡**ï¼š{self.stats['duplicate_news']} æ¡
- **åˆå¹¶æ–°é—»æ•°é‡**ï¼š{self.stats['merged_news']} æ¡
- **ä½ä»·å€¼æ–°é—»ï¼ˆå·²è¿‡æ»¤ï¼‰**ï¼š{self.stats['low_value_news']} æ¡

---

## ğŸ† è¯„åˆ†æ’è¡Œæ¦œ

### æœ€é«˜ä»·å€¼æ–°é—»ï¼ˆTOP 10ï¼‰

"""

        for i, news_record in enumerate(news_records[:10], 1):
            name = news_record['sources'][0]['name']
            score = news_record['value']['score']
            title = news_record['title']
            filename = self.sanitize_filename(name) + f"-{score}.md"

            markdown += f"""
### {i}. [{title}]({news_record['url']})
- **ä»·å€¼è¯„åˆ†**ï¼š{score} / 100
- **æ¥æº**ï¼š{name}
- **è¯¦æƒ…**ï¼š[`{filename}`](news/{self.today}/{filename})

"""

        markdown += """
---

## ğŸ“‚ é«˜ä»·å€¼æ–°é—»åˆ—è¡¨

"""

        for news_record in news_records:
            name = news_record['sources'][0]['name']
            score = news_record['value']['score']
            title = news_record['title']
            filename = self.sanitize_filename(name) + f"-{score}.md"

            markdown += f"### [{title}]({news_record['url']})\n\n"
            markdown += f"- **ä»·å€¼è¯„åˆ†**ï¼š{score} / 100\n"
            markdown += f"- **æ¥æº**ï¼š{name}\n"
            markdown += f"- **è¯¦æƒ…**ï¼š[`{filename}`](news/{self.today}/{filename})\n\n"

        markdown += """
---

## ğŸ”— æ¥æºç»Ÿè®¡

- **æœºå™¨ä¹‹å¿ƒ**ï¼š{len([n for n in news_records if 'æœºå™¨ä¹‹å¿ƒ' in s['name'] for s in n['sources']])} æ¡
- **AI å‰çº¿**ï¼š{len([n for n in news_records if 'AI å‰çº¿' in s['name'] for s in n['sources']])} æ¡
- **36æ°ª AI é¢‘é“**ï¼š{len([n for n in news_records if '36æ°ª' in s['name'] for s in n['sources']])} æ¡
- **æ™ºä¸œè¥¿**ï¼š{len([n for n in news_records if 'æ™ºä¸œè¥¿' in s['name'] for s in n['sources']])} æ¡
- **å…¶ä»–**ï¼š{len([n for n in news_records if not any(x in s['name'] for x in ['æœºå™¨ä¹‹å¿ƒ', 'AI å‰çº¿', '36æ°ª', 'æ™ºä¸œè¥¿']) for s in n['sources']])} æ¡

---

## ğŸ’¡ ä»Šæ—¥é‡ç‚¹

### ğŸ”¥ æœ€é«˜ä»·å€¼æ–°é—»

"""

        if news_records:
            top_news = news_records[0]
            name = top_news['sources'][0]['name']
            score = top_news['value']['score']
            title = top_news['title']

            markdown += f"""
**{title}**

**ä»·å€¼è¯„åˆ†**ï¼š{score} / 100
**æ¥æº**ï¼š{name}

**ä¸ºä»€ä¹ˆé‡è¦**ï¼š
- æƒå¨æ¥æº
- å†…å®¹é«˜è´¨é‡
- æ—¶æ•ˆæ€§å¼º
- å½±å“å¹¿æ³›

---

## ğŸ”® è¶‹åŠ¿é¢„æµ‹

### ä¸‹å°æ—¶è¶‹åŠ¿

1. **AI Agent æ™®åŠ**ï¼šæ›´å¤šä¼ä¸šå°†é‡‡ç”¨ AI Agent è‡ªåŠ¨åŒ–å·¥ä½œæµ
2. **å¤šæ¨¡æ€ AI æˆç†Ÿ**ï¼šå¤šæ¨¡æ€ AI æŠ€æœ¯å°†æ›´åŠ æˆç†Ÿå’Œæ™®åŠ
3. **AI è§„èŒƒåŒ–**ï¼šAI è¡Œä¸šå°†å»ºç«‹æ›´å¤šçš„è§„èŒƒå’Œæ ‡å‡†

---

## ğŸ“ æ›´æ–°è¯´æ˜

- **æ›´æ–°é¢‘ç‡**ï¼šæ¯å°æ—¶æ›´æ–°ä¸€æ¬¡
- **è¿‡æ»¤æ ‡å‡†**ï¼šä»·å€¼è¯„åˆ† >= 70 çš„é«˜ä»·å€¼æ–°é—»
- **å»é‡æœºåˆ¶**ï¼šåŸºäºé“¾æ¥ã€æ ‡é¢˜ã€å†…å®¹çš„å¤šçº§å»é‡
- **åˆå¹¶æœºåˆ¶**ï¼šåŸºäºå…³é”®è¯ç›¸ä¼¼åº¦çš„è‡ªåŠ¨åˆå¹¶
- **ä»·å€¼è¯„ä¼°**ï¼šå¤šç»´åº¦è¯„åˆ†ï¼ˆæ¥æºã€æ ‡é¢˜ã€å†…å®¹ã€æ—¶é—´ï¼‰

---

*æœ¬å†…å®¹ç”± AI Sense ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
*æœ€åæ›´æ–°ï¼š{self.today} 08:50*
*é«˜ä»·å€¼æ–°é—»æ•°é‡ï¼š{len(news_records)} æ¡*
"""

        # ä¿å­˜æ±‡æ€»æ–‡ä»¶
        summary_path = os.path.join(self.news_dir, f"{self.today}-summary.md")
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(markdown)

        logger.info(f"å·²ç”Ÿæˆæ±‡æ€»æ–‡ä»¶ï¼š{summary_path}")

    def print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        logger.info("\n========================================")
        logger.info("æŠ“å–å®Œæˆï¼")
        logger.info("========================================")
        logger.info(f"æ€»æŠ“å–æ•°é‡ï¼š{self.stats['total_fetched']} æ¡")
        logger.info(f"å”¯ä¸€æ–°é—»æ•°é‡ï¼š{self.stats['unique_news']} æ¡")
        logger.info(f"é‡å¤æ–°é—»æ•°é‡ï¼š{self.stats['duplicate_news']} æ¡")
        logger.info(f"åˆå¹¶æ–°é—»æ•°é‡ï¼š{self.stats['merged_news']} æ¡")
        logger.info(f"ä½ä»·å€¼æ–°é—»ï¼ˆå·²è¿‡æ»¤ï¼‰ï¼š{self.stats['low_value_news']} æ¡")
        logger.info(f"é«˜ä»·å€¼æ–°é—»ï¼š{self.stats['high_value_news']} æ¡")
        logger.info("========================================")


def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºæ™ºèƒ½æŠ“å–å™¨
        fetcher = IntelligentNewsFetcher()

        # æŠ“å–æ‰€æœ‰æ¥æº
        fetcher.fetch_all()

        logger.info("âœ… æ™ºèƒ½æŠ“å–å®Œæˆï¼")

        return 0

    except Exception as e:
        logger.error(f"âŒ æ™ºèƒ½æŠ“å–å¤±è´¥ï¼š{e}")
        import traceback
        logger.error(traceback.format_exc())
        return 1


if __name__ == '__main__':
    sys.exit(main())
